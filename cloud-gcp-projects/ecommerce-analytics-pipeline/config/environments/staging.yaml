# Staging Environment Configuration
# E-commerce Analytics Pipeline

environment: "staging"
gcp:
  project_id: "your-staging-project-id"
  region: "us-central1"
  zone: "us-central1-a"
  service_account_key_path: "config/credentials/staging-service-account.json"
  use_default_credentials: false

bigquery:
  dataset_id: "ecommerce_analytics_staging"
  project_id: "your-staging-project-id"
  location: "US"
  default_table_expiration_ms: 5184000000  # 60 days
  clustering_fields: ["date", "customer_id", "product_id"]

storage:
  bucket_name: "ecommerce-analytics-staging-raw-data"
  processed_bucket: "ecommerce-analytics-staging-processed"
  curated_bucket: "ecommerce-analytics-staging-curated"
  retention_days: 90
  compression: "gzip"

data_sources:
  customer_orders:
    type: "csv"
    path: "data/raw/customer_orders.csv"
    encoding: "utf-8"
    delimiter: ","
    header: true
  
  product_catalog:
    type: "csv"
    path: "data/raw/product_catalog.csv"
    encoding: "utf-8"
    delimiter: ","
    header: true
  
  web_analytics:
    type: "json"
    path: "data/raw/web_analytics.json"
    api_endpoint: "https://api.staging.com/analytics"
    api_key: "${WEB_ANALYTICS_API_KEY}"
  
  inventory:
    type: "csv"
    path: "data/raw/inventory.csv"
    encoding: "utf-8"
    delimiter: ","
    header: true

api_config:
  timeout_seconds: 45
  retry_attempts: 4
  retry_delay_seconds: 8
  rate_limit_per_minute: 500

data_quality:
  validation_rules:
    customer_orders:
      required_fields: ["order_id", "customer_id", "order_date", "total_amount"]
      data_types:
        order_id: "string"
        customer_id: "string"
        order_date: "datetime"
        total_amount: "float"
      constraints:
        total_amount: ">= 0"
        order_date: ">= 2020-01-01"
    
    product_catalog:
      required_fields: ["product_id", "product_name", "category", "price"]
      data_types:
        product_id: "string"
        product_name: "string"
        category: "string"
        price: "float"
      constraints:
        price: ">= 0"
  
  quality_threshold: 0.96
  enable_auto_correction: true
  log_quality_issues: true

business_rules:
  customer_segments:
    high_value: "total_spent >= 1000"
    medium_value: "total_spent >= 500 AND total_spent < 1000"
    low_value: "total_spent < 500"
  
  product_categories:
    electronics: ["laptop", "phone", "tablet", "accessories"]
    clothing: ["shirt", "pants", "dress", "shoes"]
    home: ["furniture", "decor", "kitchen", "garden"]
  
  revenue_calculations:
    gross_revenue: "sum(total_amount)"
    net_revenue: "sum(total_amount - discount_amount)"
    average_order_value: "avg(total_amount)"

aggregation:
  time_periods:
    - "daily"
    - "weekly"
    - "monthly"
    - "quarterly"
    - "yearly"
  
  customer_metrics:
    - "total_orders"
    - "total_spent"
    - "average_order_value"
    - "last_order_date"
    - "customer_lifetime_value"
  
  product_metrics:
    - "total_sold"
    - "total_revenue"
    - "average_rating"
    - "inventory_turnover"

loading:
  bigquery_write_disposition: "WRITE_TRUNCATE"
  storage_format: "parquet"
  enable_partitioning: true
  partition_field: "date"
  clustering_enabled: true
  
  materialized_views:
    refresh_interval_ms: 1200000  # 20 minutes
    enable_auto_refresh: true

pipeline:
  log_level: "INFO"
  enable_monitoring: true
  enable_alerting: true
  max_retries: 4
  timeout_minutes: 90
  
  scheduling:
    extraction_interval_minutes: 10
    transformation_interval_minutes: 20
    loading_interval_minutes: 30
  
  parallel_processing:
    max_workers: 8
    chunk_size: 2500
    enable_streaming: true

monitoring:
  cloud_monitoring:
    enabled: true
    project_id: "your-staging-project-id"
    custom_metrics: true
  
  alerting:
    enabled: true
    slack_webhook: "${SLACK_WEBHOOK_URL}"
    email_recipients: ["data-team@company.com"]
    
    rules:
      - name: "Pipeline Failure"
        condition: "status == 'failed'"
        severity: "error"
        description: "ETL pipeline execution failed"
      
      - name: "Data Quality Below Threshold"
        condition: "quality_score < 0.96"
        severity: "warning"
        description: "Data quality score below staging threshold"
      
      - name: "High Error Rate"
        condition: "error_rate > 0.03"
        severity: "warning"
        description: "Error rate exceeds 3% threshold"
      
      - name: "Pipeline Performance Degradation"
        condition: "execution_time > 90"
        severity: "info"
        description: "Pipeline execution time exceeds 90 minutes"
  
  health_checks:
    enabled: true
    check_interval_minutes: 10
    timeout_seconds: 45

security:
  encryption:
    at_rest: true
    in_transit: true
  
  access_control:
    principle_of_least_privilege: true
    service_account_rotation_days: 60
  
  audit_logging:
    enabled: true
    log_level: "INFO"
    retention_days: 180

testing:
  unit_tests:
    enabled: true
    coverage_threshold: 0.85
  
  integration_tests:
    enabled: true
    test_data_size: "medium"
  
  data_quality_tests:
    enabled: true
    sample_size: 5000
  
  performance_tests:
    enabled: true
    timeout_seconds: 450

backup:
  enabled: true
  retention_days: 180
  backup_interval_hours: 48
  cross_region: false
